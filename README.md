# Image Compression using Autoencoders & Variational Autoencoders

A comprehensive project exploring neural network-based image compression techniques using Autoencoders (AE) and Variational Autoencoders (VAE).

## ğŸ“Œ Theory

### Autoencoders (AE)
- Neural networks trained to reconstruct input data
- **Encoder** compresses the input into a latent representation
- **Decoder** reconstructs the data from this compressed form
- **Deterministic**: same input â†’ same latent code â†’ same reconstruction
- Good for direct compression but latent space may not be continuous or structured

### Variational Autoencoders (VAE)
- A probabilistic extension of autoencoders
- Encoder outputs a **distribution** (mean & variance), not a fixed latent code
- Latent space is smooth and continuous â€” good for interpolation and sampling
- Reconstructions may be blurrier due to random sampling, but representation is more robust
- Useful for compression + generative modeling

## ğŸ” Key Differences in Compression


| Aspect | Autoencoder (AE) | Variational Autoencoder (VAE) |
|--------|------------------|-------------------------------|
| **Latent Code** | Deterministic vector z | Distribution with Î¼, Ïƒ |
| **Loss** | MSE: \|\|x - xÌ‚\|\|Â² | ELBO: Reconstruction + KL divergence |
| **Compression Quality** | Sharper, high-fidelity | Slightly blurrier |
| **Latent Space** | May be irregular | Smooth, continuous, structured |
| **Extra Benefit** | Direct compression | Compression + generation |




### Autoencoder (AE)

An encoder f_Î¸(x) maps input x to a latent code z.

A decoder g_Ï†(z) reconstructs the input.

```
z = f_Î¸(x),  xÌ‚ = g_Ï†(z)
```

**Loss function (Reconstruction Loss):**

```
L_AE(x, xÌ‚) = ||x - xÌ‚||Â²
```

This enforces that the reconstructed image xÌ‚ is close to the original x.

### Variational Autoencoder (VAE)

Instead of mapping to a fixed latent code, the encoder outputs a distribution:

```
q_Ï†(z|x) = N(z; Î¼(x), ÏƒÂ²(x)I)
```

A latent sample is drawn using the reparameterization trick:

```
z = Î¼(x) + Ïƒ(x) âŠ™ Îµ,  Îµ ~ N(0, I)
```

The decoder reconstructs from z:

```
xÌ‚ = g_Î¸(z)
```

**Loss function (Evidence Lower Bound, ELBO):**

```
L_VAE(x, xÌ‚) = E_q_Ï†(z|x)[-log p_Î¸(x|z)] + D_KL(q_Ï†(z|x) || p(z))
```

where:
- First term: **Reconstruction Loss**
- Second term: **Regularization** (KL divergence)
- p(z) = N(0, I) is the prior


## ğŸ“Š Notebooks & Insights

### `ImageCompression_AutoEncoderModel.ipynb`
- Implements a standard Autoencoder for image compression
- Reconstructions are sharp and closely match input images

### `VAE_ImageCompression.ipynb`
- Implements a Variational Autoencoder
- Reconstructions are slightly blurrier, but the model learns a smooth latent space

### `ImageCompression.ipynb`
- Supporting notebook for preprocessing and baseline compression experiments

## ğŸ”¥ Key Takeaways

- **AE** = better for sharp, high-fidelity compression
- **VAE** = better for structured latent space & generative tasks, but with some quality trade-off


## ğŸ“ˆ Expected Results

### Autoencoder Results
- High-fidelity reconstructions
- Sharp image quality
- Direct compression ratio control

### VAE Results
- Smooth latent space representation
- Generative capabilities
- Slightly blurrier but more robust reconstructions
